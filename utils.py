import streamlit as st

def test_label(label):
    """
    Check if the predicted label belongs to the expected categories.

    Args:
        label (string): Label returned by the model

    Returns:
        'True' if the label belongs to the categories; 'False' otherwise.
    """

    if label in ["criticism", "contempt", "defensiveness", "stonewalling", "neutral", "unclear"]:
        return True
    else:
        return False


def call_api_client(llm_model, system_prompt, user_text, temperature):
    """
    Call the API and get the response.

    Args:
        llm_model (string): Name of the OpenAI model
        system_prompt (string): A prompt used to inform the model how to modify the user text.
        user_text: Original text provided by the user.
        temperature (float): Temperature for the OpenAI model

    Returns:
        response (string): response from api (a label, edited text, or some text)
        token_usage (tuple): the number of tokens used
    """

    completion = st.session_state.client_openai.chat.completions.create(
        model=llm_model,
        messages=[
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_text}
        ],
        temperature=temperature,
    )

    response = completion.choices[0].message.content

    token_usage = count_token_usage(completion)

    return response, token_usage

def count_token_usage(api_response):
    """
    Count how many token have been sent to the API.
    This version is specific for OpenAI and Streamlit.

    Args:
        api_response: Object returned by 'streamlit.session_state.client_openai.chat.completions.create()'

    Returns:
        (tuple): the number of tokens used, that is
            - 'completion_tokens': token generated by the model.
            - 'prompt_tokens': tokens sent to the api.
            - 'total_tokens': total count of used tokens.
    """

    completion_tokens = api_response.usage.completion_tokens
    prompt_tokens = api_response.usage.prompt_tokens
    total_tokens = api_response.usage.total_tokens

    return (completion_tokens, prompt_tokens, total_tokens)
