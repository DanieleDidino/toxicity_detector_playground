from openai import OpenAI
import streamlit as st
from prompts import create_prompt_clf, create_prompt_edit
from utils import test_label, call_api_client

####################################################################################
# Config

# About text for the menu item
about = """
This app is a playground to experiment with LLM to detect toxic/violent language and convert it to neutral text
"""

# streamlit config
st.set_page_config(
    page_title="ToxLang Playground",
    layout="wide",
    menu_items={
        "About": about
    }
)

st.header("Enter a sentence to evaluate")

# Condense the layout
padding = 0
st.markdown(
    f""" <style>
    .reportview-container .main .block-container{{
        padding-top: {padding}rem;
        padding-right: {padding}rem;
        padding-left: {padding}rem;
        padding-bottom: {padding}rem;
    }} </style> """,
    unsafe_allow_html=True,
)

# load custom css styles
with open(".streamlit/custom.css") as f:
    st.markdown(f"<style>{f.read()}</style>", unsafe_allow_html=True)

# Dictionary for LLM selection
llm_available = {
    "Open AI: gpt-3.5-turbo":"gpt-3.5-turbo",
    "Open AI: gpt-4": "gpt-4",
}

# LLM temperature
TEMPERATURE = 0

# System content for classifying toxic language
system_content_clf = create_prompt_clf()

# System content for converting toxic language into neutral language
# system_content_edit = create_prompt_edit()

####################################################################################
# Left column

sidebar = st.sidebar

with sidebar:

    # Custom page title and subtitle
    st.title("ToxLang")
    st.markdown("<br>", unsafe_allow_html=True)
    st.subheader("Detect and correct toxic language", divider="orange")
    st.markdown("<br>", unsafe_allow_html=True)

    # Get OpenAI ley from user
    openai_label = "Enter your [OpenAi key](https://platform.openai.com/account/api-keys)"
    OPENAI_KEY = st.text_input(label=openai_label, type="password", help="Enter your OpenAi key")

    # Add space between elements of the column
    st.markdown("<br>", unsafe_allow_html=True)
    
    # Initialize chat history
    if "messages" not in st.session_state:
        st.session_state.messages = []

    # Selectbox with the list of LLM
    help_selectbox_llm = "This option is for when we will use open-source LLM"
    selected_llm = st.selectbox("Select a LLM", options=llm_available.keys(), help=help_selectbox_llm)
    if selected_llm and OPENAI_KEY:
        LLM_MODEL = llm_available[selected_llm]
        # Initialize LLM
        if "client_openai" not in st.session_state:
            st.session_state.client_openai = OpenAI(api_key=OPENAI_KEY)
    
    # Add space between elements of the column
    st.markdown("<br>", unsafe_allow_html=True)

    # Initialize edit_text (define whether or not convert the toxic to neutral language)
    if "edit_text" not in st.session_state:
        st.session_state.edit_text = False
    
    # Toggle to select whether convert the original text
    help_toggle_edit_text = "Conver the sentence to neutral language?"
    st.session_state.edit_text = st.toggle("Convert to neutral language?", value=False, help=help_toggle_edit_text)

####################################################################################
# Chat

# Display chat messages from history on app rerun
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"], unsafe_allow_html=True)
    
# React to user input
if user_text := st.chat_input("How may I help you?"):
    # Display user message in chat message container
    st.chat_message("user").markdown(user_text)
    # Add user message to chat history
    st.session_state.messages.append({"role": "user", "content": user_text})

    if OPENAI_KEY:
        # Detect toxic language
        selected_label, token_usage = call_api_client(LLM_MODEL, system_content_clf, user_text, TEMPERATURE)
        completion_tokens, prompt_tokens, total_tokens = token_usage
        # Check if the label generated by the model belongs to the list of expected categories
        is_label_in_expected_categories = test_label(selected_label)

        if is_label_in_expected_categories:
            # The model chose a label that is in the list of expected categories

            # Edit text
            if st.session_state.edit_text and selected_label!="unclear" and selected_label!="neutral":
                system_content_edit = create_prompt_edit(selected_label)
                edited_text, token_usage = call_api_client(LLM_MODEL, system_content_edit, user_text, TEMPERATURE)
                edited_text = f"<br><br>Suggested correction:<br><font style='background-color: #FFFF00'>{edited_text}</font>"
                completion_tokens_edit, prompt_tokens_edit, total_tokens_edit = token_usage
                completion_tokens += completion_tokens_edit
                prompt_tokens += prompt_tokens_edit
                total_tokens += prompt_tokens_edit

            else:
                edited_text = ""
            
            # Dict with token usage
            used_tokens = {
                "completion_tokens":completion_tokens,
                "prompt_tokens":prompt_tokens,
                "total_tokens":total_tokens
            }
            
            # Prepare response for user
            response_for_user = f"This sentence is labeled as <b style='color: red'>{selected_label}</b><br>(token usage: {used_tokens}){edited_text}"

        else:
            # The model chose a label that is not in the list of expected categories
            response_for_user = "Something went wrong, the model choose an unexpected category!"

    else:
        response_for_user = "Please add your OpenAI key to continue."

    # Display response in chat message container
    with st.chat_message("assistant"):
        st.markdown(response_for_user, unsafe_allow_html=True)
    # Add response to chat history
    st.session_state.messages.append({"role": "assistant", "content": response_for_user})
